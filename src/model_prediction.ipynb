{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e828096",
   "metadata": {},
   "source": [
    "# Vehicle Sale Price Prediction\n",
    "\n",
    "This notebook documents the full end-to-end process for predicting vehicle sale prices (`Sold_Amount`). It covers data understanding, feature engineering, modeling, evaluation, and experiment tracking with mlflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddd1d6",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "The dataset is provided as a `.rpt` file, which is a tab-delimited text export. We load it using pandas with explicit handling of missing values. A `load_rpt` function is imported from module in `data_handler.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0ed503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[0]  # notebooks → project root\n",
    "sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046c12da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>MakeCode</th>\n",
       "      <th>FamilyCode</th>\n",
       "      <th>YearGroup</th>\n",
       "      <th>MonthGroup</th>\n",
       "      <th>SequenceNum</th>\n",
       "      <th>Description</th>\n",
       "      <th>CurrentRelease</th>\n",
       "      <th>ImportFlag</th>\n",
       "      <th>...</th>\n",
       "      <th>PrivateMax</th>\n",
       "      <th>NewPrice</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Branch</th>\n",
       "      <th>SaleCategory</th>\n",
       "      <th>Sold_Date</th>\n",
       "      <th>Compliance_Date</th>\n",
       "      <th>Age_Comp_Months</th>\n",
       "      <th>KM</th>\n",
       "      <th>Sold_Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Holden</td>\n",
       "      <td>Commodore</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>COMMODO</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>VE Omega Sedan 4dr. Auto 4sp 3.6i</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>34790.0</td>\n",
       "      <td>White</td>\n",
       "      <td>Perth (WA)</td>\n",
       "      <td>Auction</td>\n",
       "      <td>2015-11-03 00:00:00.000</td>\n",
       "      <td>02/2008</td>\n",
       "      <td>93.0</td>\n",
       "      <td>227878.0</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Holden</td>\n",
       "      <td>Commodore</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>COMMODO</td>\n",
       "      <td>1993</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>VR Executive Wagon 5dr. Auto 4sp 3.8i</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>27978.0</td>\n",
       "      <td>Red</td>\n",
       "      <td>Belmore (NSW)</td>\n",
       "      <td>Auction</td>\n",
       "      <td>2000-10-18 00:00:00.000</td>\n",
       "      <td>08/1993</td>\n",
       "      <td>86.0</td>\n",
       "      <td>153091.0</td>\n",
       "      <td>6800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>RAV4</td>\n",
       "      <td>TOYO</td>\n",
       "      <td>RAV4</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>ACA33R MY12 CV Wagon 5dr Man 5sp 4x4 2.4i</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>15800.0</td>\n",
       "      <td>31990.0</td>\n",
       "      <td>040 - Glacier White (T)</td>\n",
       "      <td>Sunshine (VIC)</td>\n",
       "      <td>Dealer Only Auction</td>\n",
       "      <td>2014-02-05 00:00:00.000</td>\n",
       "      <td>10/2012</td>\n",
       "      <td>16.0</td>\n",
       "      <td>27374.0</td>\n",
       "      <td>22900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Holden</td>\n",
       "      <td>Commodore</td>\n",
       "      <td>HOLD</td>\n",
       "      <td>COMMODO</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>VZ@VE MY07 Executive Wagon 5dr. Auto 4sp 3.6i</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>4800.0</td>\n",
       "      <td>35990.0</td>\n",
       "      <td>Quicksilver</td>\n",
       "      <td>Belmore (NSW)</td>\n",
       "      <td>Auction</td>\n",
       "      <td>2011-01-10 00:00:00.000</td>\n",
       "      <td>01/2007</td>\n",
       "      <td>48.0</td>\n",
       "      <td>99452.0</td>\n",
       "      <td>10500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Tarago</td>\n",
       "      <td>TOYO</td>\n",
       "      <td>TARAGO</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ACR50R GLi Wagon 8st 5dr Spts Auto 4sp 2.4i</td>\n",
       "      <td>F</td>\n",
       "      <td>L</td>\n",
       "      <td>...</td>\n",
       "      <td>12400.0</td>\n",
       "      <td>49490.0</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Hobart (TAS)</td>\n",
       "      <td>Special Fixed Price</td>\n",
       "      <td>2009-05-23 00:00:00.000</td>\n",
       "      <td>01/2007</td>\n",
       "      <td>28.0</td>\n",
       "      <td>44355.0</td>\n",
       "      <td>31320.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 130 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Make      Model MakeCode FamilyCode  YearGroup  MonthGroup  SequenceNum  \\\n",
       "0  Holden  Commodore     HOLD    COMMODO       2008           0            0   \n",
       "1  Holden  Commodore     HOLD    COMMODO       1993           7           41   \n",
       "2  Toyota       RAV4     TOYO       RAV4       2012           0            6   \n",
       "3  Holden  Commodore     HOLD    COMMODO       2007           0           11   \n",
       "4  Toyota     Tarago     TOYO     TARAGO       2007           0            0   \n",
       "\n",
       "                                     Description CurrentRelease ImportFlag  \\\n",
       "0              VE Omega Sedan 4dr. Auto 4sp 3.6i              F          L   \n",
       "1          VR Executive Wagon 5dr. Auto 4sp 3.8i              F          L   \n",
       "2      ACA33R MY12 CV Wagon 5dr Man 5sp 4x4 2.4i              F          L   \n",
       "3  VZ@VE MY07 Executive Wagon 5dr. Auto 4sp 3.6i              F          L   \n",
       "4    ACR50R GLi Wagon 8st 5dr Spts Auto 4sp 2.4i              F          L   \n",
       "\n",
       "   ... PrivateMax NewPrice                   Colour          Branch  \\\n",
       "0  ...     5500.0  34790.0                    White      Perth (WA)   \n",
       "1  ...     2000.0  27978.0                      Red   Belmore (NSW)   \n",
       "2  ...    15800.0  31990.0  040 - Glacier White (T)  Sunshine (VIC)   \n",
       "3  ...     4800.0  35990.0              Quicksilver   Belmore (NSW)   \n",
       "4  ...    12400.0  49490.0                   Silver    Hobart (TAS)   \n",
       "\n",
       "          SaleCategory                Sold_Date Compliance_Date  \\\n",
       "0              Auction  2015-11-03 00:00:00.000         02/2008   \n",
       "1              Auction  2000-10-18 00:00:00.000         08/1993   \n",
       "2  Dealer Only Auction  2014-02-05 00:00:00.000         10/2012   \n",
       "3              Auction  2011-01-10 00:00:00.000         01/2007   \n",
       "4  Special Fixed Price  2009-05-23 00:00:00.000         01/2007   \n",
       "\n",
       "  Age_Comp_Months        KM Sold_Amount  \n",
       "0            93.0  227878.0      2000.0  \n",
       "1            86.0  153091.0      6800.0  \n",
       "2            16.0   27374.0     22900.0  \n",
       "3            48.0   99452.0     10500.0  \n",
       "4            28.0   44355.0     31320.0  \n",
       "\n",
       "[5 rows x 130 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from data.data_handler import load_rpt\n",
    "\n",
    "df_train = load_rpt(r'../data/DatiumTrain.rpt')\n",
    "df_test = load_rpt(r'../data/DatiumTest.rpt')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cfab46",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Quality Checks\n",
    "\n",
    "We inspect missing values, target variable and potential data quality issues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6200433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50704 entries, 0 to 50703\n",
      "Columns: 130 entries, Make to Sold_Amount\n",
      "dtypes: float64(65), int64(6), object(59)\n",
      "memory usage: 50.3+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AltEngTorqueFrom          1.000000\n",
       "QuickChargeVoltage        1.000000\n",
       "QuickChargeMins           1.000000\n",
       "NormalChargeMins          1.000000\n",
       "NormalChargeVoltage       1.000000\n",
       "KMRangeElectricEng        0.999961\n",
       "TopSpeedElectricEng       0.999961\n",
       "AltEngAmpHours            0.999882\n",
       "ElectricEngineLocation    0.999487\n",
       "FreeScheduledService      0.998107\n",
       "AltEngPowerFrom           0.996923\n",
       "AltEngTorqueTo            0.996766\n",
       "PowerRPMFrom              0.995878\n",
       "AltEngPowerTo             0.995326\n",
       "Roofline                  0.989744\n",
       "AltEngTorque              0.984518\n",
       "AltEngDrive               0.983532\n",
       "AltEngVolts               0.983512\n",
       "AltEngPower               0.983512\n",
       "AltEngCurrentType         0.983473\n",
       "dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.info()\n",
    "\n",
    "missing_ratio = df_train.isnull().mean().sort_values(ascending=False)\n",
    "missing_ratio.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c470bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     50700.000000\n",
       "mean      16401.722226\n",
       "std       10312.029249\n",
       "min           0.000000\n",
       "25%       10500.000000\n",
       "50%       15000.000000\n",
       "75%       20425.000000\n",
       "max      317000.000000\n",
       "Name: Sold_Amount, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Sold_Amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afd0ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     11488.000000\n",
       "mean      18142.115396\n",
       "std       11392.162124\n",
       "min         185.000000\n",
       "25%       11200.000000\n",
       "50%       15750.000000\n",
       "75%       22500.000000\n",
       "max      165450.000000\n",
       "Name: Sold_Amount, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Sold_Amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808064ca",
   "metadata": {},
   "source": [
    "The target variable seems to be highly right skewed, ranging from 185 to 160k in values. For some models it might affect model (like linear regression), due to highly biased coefficients towards high proportion values. One method can be transforming into 'natural logarithm' values, depending on the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b24c29",
   "metadata": {},
   "source": [
    "## 3. Feature Selection & null data handling\n",
    "\n",
    "3.1 Drop some unused columns:\n",
    "\n",
    "The following columns are explicitly excluded, as per instruction:\n",
    "\n",
    "    AvgWholesale\n",
    "    AvgRetail\n",
    "    GoodWholesale\n",
    "    GoodRetail\n",
    "    TradeMin\n",
    "    TradeMax\n",
    "    PrivateMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f061da",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_cols = [\n",
    "    'AvgWholesale','AvgRetail','GoodWholesale','GoodRetail',\n",
    "    'TradeMin','TradeMax','PrivateMax'\n",
    "]\n",
    "\n",
    "df_train = df_train.drop(columns=[c for c in unused_cols if c in df_train.columns])\n",
    "df_test = df_test.drop(columns=[c for c in unused_cols if c in df_test.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c11a7b",
   "metadata": {},
   "source": [
    "3.2 From observations on the data, Columns below are to be dropped:\n",
    "\n",
    "    Description\n",
    "    VIN\n",
    "    ExtraIdentification\n",
    "    ModelCode\n",
    "    MakeCode\n",
    "    FamilyCode\n",
    "    Series\n",
    "    Branch\n",
    "    SequenceNum\n",
    "    SeriesModelYear\n",
    "    DriveDescription\n",
    "    DriveCode\n",
    "\n",
    "They have thousands of unique values, and are mostly identifiers, which may not be useful for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"VIN\", \"EngineNum\", \"Description\", \"ExtraIdentification\",\n",
    "    \"ModelCode\", \"MakeCode\", \"FamilyCode\",\n",
    "    \"Series\", \"Branch\", \"SequenceNum\", \"SeriesModelYear\", \"DriveDescription\",'DriveCode',\n",
    "]\n",
    "\n",
    "df_train = df_train.drop(columns=[c for c in drop_cols if c in df_train.columns])\n",
    "df_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c267121f",
   "metadata": {},
   "source": [
    "3.3 Merge text columns that are identical\n",
    "\n",
    "Some of the columns are just texts, and potentially can be merged into one. In this run, the 'badge' description is merged into a column called 'Car_Description'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ef8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_columns = ['BadgeDescription', 'BadgeSecondaryDescription', 'BadgeDescription', 'BodyStyleDescription', 'BodyConfigDescription']\n",
    "\n",
    "\n",
    "new_col = 'Car_Description'\n",
    "\n",
    "# Merge, and handle NaN with blank space\n",
    "for df in [df_train, df_test]:\n",
    "    df[new_col] = (\n",
    "        df[merge_columns]\n",
    "        .fillna(\"\")\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .str.strip()\n",
    "    )\n",
    "    df = df.drop(columns=merge_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67482a70",
   "metadata": {},
   "source": [
    "3.4 Remove columns that have a lot of empty values or NaN (Can be optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d84c76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 0 columns in training due to >90.0% missing: []\n",
      "Dropping 0 columns in test due to >90.0% missing: []\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.9  # Set threshold 90%\n",
    "for df in [df_train]:\n",
    "    # Compute the fraction of missing values per column\n",
    "    missing_frac = df.isna().sum() / len(df)\n",
    "    # Drop columns with missing fraction above threshold\n",
    "    drop_cols = missing_frac[missing_frac > threshold].index.tolist()\n",
    "    print(f\"Dropping {len(drop_cols)} columns in training due to >{threshold*100}% missing:\", drop_cols)\n",
    "    df.drop(columns=drop_cols, inplace=True)\n",
    "print(f\"Dropping {len(drop_cols)} columns in test due to >{threshold*100}% missing:\", drop_cols)\n",
    "df_test.drop(columns=drop_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7289541",
   "metadata": {},
   "source": [
    "3.5 Handle numeric and categorical columns - null values\n",
    "\n",
    "Numeric columns - Use Imputations, in this example ** Median ** is used for imputation. Reason is it is more robust than Mean/Mode, in terms of outliers or heavy skewed distributions. Some methods like KNN can be used, for simplicity, median is used in this run\n",
    "\n",
    "Categorical columns - Replace NaN with 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be9cbd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1213: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train NaNs remaining: []\n",
      "Test NaNs remaining : []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # 1. Ensure numeric columns are numeric\n",
    "    # num_cols = df.select_dtypes(include=['int64','float64','float32','int32']).columns.tolist()\n",
    "    num_cols = df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "    for col in num_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')  # convert errors to NaN\n",
    "\n",
    "    # 2. Fill numeric columns with median\n",
    "    for col in num_cols:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "    # 3. Fill categorical columns with 'Unknown'\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "\n",
    "train_na_cols = df_train.columns[df_train.isna().any()].tolist()\n",
    "test_na_cols = df_test.columns[df_test.isna().any()].tolist()\n",
    "\n",
    "# Columns to drop (union) - due to all entries are NaN\n",
    "cols_to_drop = list(set(train_na_cols) | set(test_na_cols))\n",
    "df_train = df_train.drop(columns=cols_to_drop, errors='ignore')\n",
    "df_test  = df_test.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Quick check for any remaining NaNs\n",
    "print(\"Train NaNs remaining:\", df_train.columns[df_train.isna().any()].tolist())\n",
    "print(\"Test NaNs remaining :\", df_test.columns[df_test.isna().any()].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769753e1",
   "metadata": {},
   "source": [
    "3.6 Check for infinity or too large values (Can be optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9decb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with infinity values:\n",
      "[]\n",
      "Columns with too large values: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Boolean mask of infinities\n",
    "inf_mask = df_test.isin([np.inf, -np.inf])\n",
    "\n",
    "# Columns with any infinity\n",
    "cols_with_inf = inf_mask.any()\n",
    "print(\"Columns with infinity values:\")\n",
    "print(cols_with_inf[cols_with_inf].index.tolist())\n",
    "\n",
    "# Threshold for float64 (roughly 1e308)\n",
    "threshold = 1e308\n",
    "\n",
    "# Find columns with values exceeding threshold\n",
    "too_large_cols = [col for col in df_test.select_dtypes(include=np.number).columns\n",
    "                  if (df_test[col].abs() > threshold).any()]\n",
    "\n",
    "print(\"Columns with too large values:\", too_large_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dded7c2",
   "metadata": {},
   "source": [
    "3.7 Collapse rare categories, that appear less than 5% of the data (threshold), and label as 'Unknown' also\n",
    "\n",
    "Columns are:\n",
    "\n",
    "    Make\n",
    "    Model\n",
    "    FuelTypeDescription \n",
    "    SaleCategory\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4601c25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_card_cols = [\n",
    "    \"Make\", \"Model\",\n",
    "    \"FuelTypeDescription\",\n",
    "    \"SaleCategory\"\n",
    "]\n",
    "\n",
    "min_freq = 0.05\n",
    "for col in high_card_cols:\n",
    "    freq = df_train[col].value_counts(normalize=True)\n",
    "    rare = freq[freq < min_freq].index\n",
    "    df_train[col] = df_train[col].replace(rare, \"Other\")\n",
    "    df_test[col] = df_test[col].where(\n",
    "        df_test[col].isin(df_train[col].unique()), \"Other\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c7a01",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Engineer additional features to capture vehicle age, usage, and power to weight ratio.\n",
    "\n",
    "Feature 1: Vehicle age, estimate from age comp months, and drop YearGroup and Compliance Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be07d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\3987853089.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Vehicle_Age_Years'] = df['Age_Comp_Months'] / 12\n",
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\3987853089.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['Vehicle_Age_Years'] = df['Age_Comp_Months'] / 12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    df['Vehicle_Age_Years'] = df['Age_Comp_Months'] / 12\n",
    "    df = df.drop(columns=['YearGroup', 'Compliance_Date'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc502ce",
   "metadata": {},
   "source": [
    "Feature 2: Usage intensity, instead of total distance travelled. Drop 'KM' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "531c070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\2112498152.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"KM_per_year\"] = df[\"KM\"] / (df['Vehicle_Age_Years'] + 0.1)\n",
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\2112498152.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"KM_per_year\"] = df[\"KM\"] / (df['Vehicle_Age_Years'] + 0.1)\n"
     ]
    }
   ],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    df[\"KM_per_year\"] = df[\"KM\"] / (df['Vehicle_Age_Years'] + 0.1)\n",
    "    df = df.drop(columns=['KM'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050baf5",
   "metadata": {},
   "source": [
    "Feature 3: Power to weight ratio, instead of raw power and weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9572092e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\3846512596.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"Power_to_weight\"] = df[\"Power\"] / df[\"KerbWeight\"]\n",
      "C:\\Users\\kbchin\\AppData\\Local\\Temp\\ipykernel_23236\\3846512596.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"Power_to_weight\"] = df[\"Power\"] / df[\"KerbWeight\"]\n"
     ]
    }
   ],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    df[\"Power_to_weight\"] = df[\"Power\"] / df[\"KerbWeight\"]\n",
    "    df = df.drop(columns=['Power', 'KerbWeight'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5534cb",
   "metadata": {},
   "source": [
    "Final training data columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74adf18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make\n",
      "Model\n",
      "MonthGroup\n",
      "CurrentRelease\n",
      "ImportFlag\n",
      "LimitedEdition\n",
      "WheelBaseConfig\n",
      "Roofline\n",
      "GearTypeDescription\n",
      "GearLocationDescription\n",
      "GearNum\n",
      "DoorNum\n",
      "EngineSize\n",
      "EngineDescription\n",
      "Cylinders\n",
      "FuelTypeDescription\n",
      "InductionDescription\n",
      "OptionCategory\n",
      "CamDescription\n",
      "EngineTypeDescription\n",
      "FuelCapacity\n",
      "FuelDeliveryDescription\n",
      "MethodOfDeliveryDescription\n",
      "GrossCombinationMAss\n",
      "GrossVehicleMass\n",
      "WheelBase\n",
      "Height\n",
      "Length\n",
      "Width\n",
      "TareMass\n",
      "PayLoad\n",
      "PowerRPMFrom\n",
      "PowerRPMTo\n",
      "Torque\n",
      "TorqueRPMFrom\n",
      "TorqueRPMTo\n",
      "RonRating\n",
      "SeatCapacity\n",
      "BuildCountryOriginDescription\n",
      "ValvesCylinder\n",
      "EngineCycleDescription\n",
      "EngineConfigurationDescription\n",
      "EngineLocation\n",
      "Acceleration\n",
      "FrontTyreSize\n",
      "RearTyreSize\n",
      "FrontRimDesc\n",
      "RearRimDesc\n",
      "TowingBrakes\n",
      "TowingNoBrakes\n",
      "WarrantyCustAssist\n",
      "FreeScheduledService\n",
      "WarrantyYears\n",
      "WarrantyKM\n",
      "FirstServiceKM\n",
      "FirstServiceMonths\n",
      "RegServiceMonths\n",
      "AltEngEngineType\n",
      "AltEngBatteryType\n",
      "AltEngCurrentType\n",
      "AltEngVolts\n",
      "AltEngChargingMethod\n",
      "AltEngPower\n",
      "AltEngPowerFrom\n",
      "AltEngPowerTo\n",
      "AltEngTorque\n",
      "AltEngTorqueTo\n",
      "AltEngDrive\n",
      "KMRangeElectricEng\n",
      "ElectricEngineLocation\n",
      "TopSpeedElectricEng\n",
      "GreenhouseRating\n",
      "AirpollutionRating\n",
      "OverallGreenStarRating\n",
      "CO2Combined\n",
      "CO2Urban\n",
      "CO2ExtraUrban\n",
      "FuelUrban\n",
      "FuelExtraurban\n",
      "FuelCombined\n",
      "EmissionStandard\n",
      "MaxEthanolBlend\n",
      "AncapRating\n",
      "VFactsClass\n",
      "VFactsSegment\n",
      "VFactsPrice\n",
      "IsPPlateApproved\n",
      "AverageKM\n",
      "GoodKM\n",
      "NewPrice\n",
      "Colour\n",
      "SaleCategory\n",
      "Sold_Date\n",
      "Age_Comp_Months\n",
      "Sold_Amount\n",
      "Car_Description\n",
      "Vehicle_Age_Years\n",
      "KM_per_year\n",
      "Power_to_weight\n"
     ]
    }
   ],
   "source": [
    "for col in df_train.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4492df",
   "metadata": {},
   "source": [
    "## 6. Data Split/Define training or test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f5a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(columns=['Sold_Amount'])\n",
    "y_train = df_train['Sold_Amount']\n",
    "\n",
    "X_test = df_test.drop(columns=['Sold_Amount'])\n",
    "y_test = df_test['Sold_Amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e989c6",
   "metadata": {},
   "source": [
    "## 7. Encoding & Model training\n",
    "\n",
    "7.0  Encoding and scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ad270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/07 18:46:24 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2026/01/07 18:46:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "2026/01/07 18:46:26 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ffb65e264bf54026965465bb671d48de', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2026/01/07 18:46:31 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2026/01/07 18:46:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "2026/01/07 18:46:32 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2026/01/07 18:47:04 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2026/01/07 18:47:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '2ad0f077103e467395bf38877997c39c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2026/01/07 18:47:17 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2026/01/07 18:47:33 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n",
      "2026/01/07 18:47:35 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.model_trainer import ModelTrainer\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "num_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# Numeric pipeline: impute NaNs with median, then scale\n",
    "# In case there were still NaNs\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute NaNs with 'Unknown', then one-hot encode\n",
    "# In case there were still NaNs\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Column transformer: apply transformers to respective columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, num_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform train, transform test\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed  = preprocessor.transform(X_test)\n",
    "y_train_processed = y_train\n",
    "y_test_processed = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d755d",
   "metadata": {},
   "source": [
    "7.1 Linear Ridge Model - Baseline\n",
    "\n",
    "Ridge regression adds a L2-penalty to conventional linear regression, so it prevents overfitting and multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9833bfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/07 18:48:09 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: {'RMSE': 57552608.43725826, 'R2 Test': 0.5565031864247749, 'R2 Train': 0.8110827219670886}\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ Initialize Ridge regression\n",
    "ridge_model = Ridge(alpha=110) # 110\n",
    "\n",
    "ridge_trainer = ModelTrainer(\"Ridge\", ridge_model, X_train_processed, y_train_processed, X_test_processed, y_test_processed)\n",
    "ridge_metrics = ridge_trainer.run()\n",
    "print(\"Ridge:\", ridge_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914be5aa",
   "metadata": {},
   "source": [
    "7.2 Random Forest - Tree based algorithm and enconding\n",
    "\n",
    "Tree-based model can perform better than polynomial regression, as it models rule-based relationship between features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/07 18:49:20 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'flatten'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest: {'RMSE': 20747286.049241293, 'R2 Test': 0.8401227068065349, 'R2 Train': 0.8103142278683197}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=12,              \n",
    "    min_samples_leaf=50,       \n",
    "    min_samples_split=100,\n",
    "    max_features=0.3,         \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_trainer = ModelTrainer(\"RandomForest\", rf, X_train_processed, y_train_processed, X_test_processed, y_test_processed)\n",
    "rf_metrics = rf_trainer.run()\n",
    "print(\"RandomForest:\", rf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbfbd6",
   "metadata": {},
   "source": [
    "7.3 LightGBM\n",
    "\n",
    "A fast computing and memory efficient tree-based model and boosting technique to improve the search compared to Random Forest model. Can be useful for large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c249f68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:912: ImportWarning: _ImportHookChainedLoader.exec_module() not found; falling back to load_module()\n",
      "<frozen importlib._bootstrap>:530: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "2026/01/07 19:03:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for lightgbm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.213286 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9017\n",
      "[LightGBM] [Info] Number of data points in the train set: 50704, number of used features: 1888\n",
      "[LightGBM] [Info] Start training from score 16401.611645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kbchin\\OneDrive\\Documents\\vehicle-sales-prediction\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2691: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "2026/01/07 19:07:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: {'RMSE': 12600330.76316419, 'R2 Test': 0.9029026364713033, 'R2 Train': 0.9404415914296298}\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgbm = LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "lgbm_trainer = ModelTrainer(\"LightGBM\", lgbm, X_train_processed, y_train_processed, X_test_processed, y_test_processed)\n",
    "lgbm_metrics = lgbm_trainer.run()\n",
    "print(\"LightGBM:\", lgbm_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad1c01",
   "metadata": {},
   "source": [
    "7.4 XGBoost\n",
    "\n",
    "XGBoost provides more fine-grained control over regularization, thus preventing overfitting. It is also a boosting tree-based model than potentially perform better than Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8659f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:912: ImportWarning: _ImportHookChainedLoader.exec_module() not found; falling back to load_module()\n",
      "<frozen importlib._bootstrap>:530: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "2026/01/07 19:08:18 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n",
      "2026/01/07 19:08:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/01/07 19:08:56 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: `_estimator_type` undefined.  Please use appropriate mixin to define estimator type.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost: {'RMSE': 13065301.043469952, 'R2 Test': 0.8993196044711536, 'R2 Train': 0.9409900467588352}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_trainer = ModelTrainer(\"XGBoost\", xgb, X_train_processed, y_train_processed, X_test_processed, y_test_processed)\n",
    "xgb_metrics = xgb_trainer.run()\n",
    "print(\"XGBoost:\", xgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f23102",
   "metadata": {},
   "source": [
    "Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d41c8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      RMSE   R2 Test  R2 Train\n",
      "Ridge         5.755261e+07  0.556503  0.811083\n",
      "RandomForest  2.074729e+07  0.840123  0.810314\n",
      "LightGBM      1.260033e+07  0.902903  0.940442\n",
      "XGBoost       1.306530e+07  0.899320  0.940990\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame.from_dict({\n",
    "    'Ridge': ridge_metrics,\n",
    "    'RandomForest': rf_metrics,\n",
    "    'LightGBM': lgbm_metrics,\n",
    "    'XGBoost': xgb_metrics\n",
    "}, orient='index')\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43931329",
   "metadata": {},
   "source": [
    "Ridge regression serves as a linear baseline and highlights the limits of additive assumptions.\n",
    "\n",
    "Random Forest helps to reduce the overfitting and seems to be able to model to non-linear relationship better than linear ridge model\n",
    "\n",
    "LightGBM and XGBoost significantly improve performance by modeling nonlinear relationships and feature interactions common in vehicle pricing such as age, mileage, and make. LightGBM performs slightly better than XGBoost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
